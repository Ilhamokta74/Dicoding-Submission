# -*- coding: utf-8 -*-
"""Sub_3_Image_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qxf3HUwq-pGfbQd__pRgN6wGj8H5UTKr

*   Nama : Ilham Oktavian
*   Dataset : Rice_Image_Dataset.zip
*   Link Dataset : https://www.muratkoklu.com/datasets/Rice_Image_Dataset.zip
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile, os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import warnings

# Extract the zip file
local_zip = './drive/MyDrive/Rice_Image_Dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

# Define directories for different rice varieties
base_dir = '/tmp/Rice_Image_Dataset'
rice_varieties = ['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']
directories = [os.path.join(base_dir, variety) for variety in rice_varieties]

# Count the number of images in each directory
total_counts = [len(os.listdir(directory)) for directory in directories]

# Print the total number of images for each rice variety
for variety, count in zip(rice_varieties, total_counts):
    print(f"Total Data {variety} Image: {count}")

# ImageDataGenerator for augmentations
val_size = 0.2

datagen = ImageDataGenerator(
    rotation_range=30,
    brightness_range=[0.2, 1.0],
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode="nearest",
    rescale=1./255,
    validation_split=val_size
)

# Train and Validation generators
train_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    color_mode="rgb",
    class_mode="categorical",
    batch_size=128,
    shuffle=True,
    subset="training"
)

validation_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    color_mode="rgb",
    class_mode="categorical",
    batch_size=128,
    shuffle=False,
    subset="validation"
)

# Model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    BatchNormalization(),
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    BatchNormalization(axis=3),
    MaxPooling2D(pool_size=(2, 2), padding='same'),
    Dropout(0.3),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(axis=3),
    MaxPooling2D(pool_size=(2, 2), padding='same'),
    Dropout(0.3),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(axis=3),
    MaxPooling2D(pool_size=(2, 2), padding='same'),
    Dropout(0.5),

    Flatten(),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.25),
    Dense(5, activation='softmax')
])

# Model Compilation
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""**Callbacks**"""

# Callbacks
models_dir = "saved_models"
if not os.path.exists(models_dir):
    os.makedirs(models_dir)

checkpointer = ModelCheckpoint(filepath='saved_models/model.hdf5',
                               monitor='val_accuracy', mode='max',
                               verbose=1, save_best_only=True)
early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001)
callbacks = [early_stopping, reduce_lr, checkpointer]

# Model Training
history = model.fit(train_generator, epochs=40, validation_data=validation_generator, callbacks=callbacks)

# Plot accuracy over epochs
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# Plot loss over epochs
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# TensorFlow Lite Conversion
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model
with tf.io.gfile.GFile('model.tflite', 'wb') as f:
    f.write(tflite_model)